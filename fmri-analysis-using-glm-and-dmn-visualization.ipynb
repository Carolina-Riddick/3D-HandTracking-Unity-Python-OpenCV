{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[{"file_id":"1KQHL98RYS5aUa0RV12WD_rDuWCCd4gjc","timestamp":1746097119065}]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13395705,"sourceType":"datasetVersion","datasetId":8500607}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/carolinariddick/fmri-analysis-using-glm-and-dmn-visualization?scriptVersionId=271517430\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# fMRI Analysis Using GLM and DMN Visualization","metadata":{"id":"pDv9FKvc7nV2"}},{"cell_type":"markdown","source":"In this results simulation, we will analyse a dataset from a â€œlisten vs. restâ€ task. Using this methodology, the researchers aimed to study the brain areas involved in sound processing.\n\nThe task is very simple: compare brain responses when people were listening to something versus when they were not.\n\nObjectives of this notebook:\n\n- Learn the basics of the nilearn library for analysing fMRI data.\n\n- Learn how to load and visualize fMRI data in Python.\n\n- Understand how to perform a statistical analysis on this type of signals.\n\n- Learn how to interpret the results.\n\n- Learn how to write a brief report summarizing the findings.","metadata":{"id":"9mima_qL-Hgf"}},{"cell_type":"markdown","source":"### Step 1: Install dependencies","metadata":{"id":"4buCk8pV_J6n"}},{"cell_type":"code","source":"!pip install --quiet --upgrade \\\n    nilearn==0.10.2 \\\n    matplotlib==3.7.2 \\\n    numpy==1.25.2 \\\n    pandas==2.1.1 \\\n    scipy==1.11.2 \\\n    scikit-learn==1.5.2 \\\n    nibabel==5.0.2 \\\n    joblib==1.3.2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:20.520568Z","iopub.execute_input":"2025-10-28T10:49:20.520949Z","iopub.status.idle":"2025-10-28T10:49:25.912733Z","shell.execute_reply.started":"2025-10-28T10:49:20.520916Z","shell.execute_reply":"2025-10-28T10:49:25.911794Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Clean the cache","metadata":{}},{"cell_type":"code","source":"!pip check --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:25.914813Z","iopub.execute_input":"2025-10-28T10:49:25.915157Z","iopub.status.idle":"2025-10-28T10:49:28.717105Z","shell.execute_reply.started":"2025-10-28T10:49:25.915131Z","shell.execute_reply":"2025-10-28T10:49:28.716109Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 2: Import dependencies","metadata":{}},{"cell_type":"code","source":"import zipfile\nimport os\nfrom nilearn.plotting import plot_anat, plot_img, plot_stat_map\nfrom nilearn.image import concat_imgs, mean_img\nfrom nilearn.glm.first_level import FirstLevelModel\nfrom nilearn.plotting import plot_design_matrix\nfrom nilearn.plotting import plot_contrast_matrix\nfrom nilearn.glm import threshold_stats_img\nfrom nilearn.image import load_img\nfrom nilearn.datasets import fetch_spm_auditory\nimport pandas as pd\nimport numpy as np\nimport matplotlib as plt\nimport os\nfrom collections import namedtuple\nimport nibabel as nib\nfrom nilearn.image import index_img\nfrom nilearn.plotting import plot_stat_map\nimport matplotlib.pyplot as plt\nfrom nilearn.reporting import get_clusters_table\nfrom nilearn import datasets, plotting","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:28.718428Z","iopub.execute_input":"2025-10-28T10:49:28.718736Z","iopub.status.idle":"2025-10-28T10:49:28.770525Z","shell.execute_reply.started":"2025-10-28T10:49:28.718707Z","shell.execute_reply":"2025-10-28T10:49:28.769761Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 3: Import dataset ","metadata":{"id":"OtmhiYMJAweA"}},{"cell_type":"markdown","source":"#### We will use the dataset I have downloaded in Kaggle.\n\nIn Nilearn, we can use datasets that are already preprocessed and ready for analysis.\nFor this task, we are going to use the fetch_spm_auditory dataset.\n\nTo import it, we can do the following:","metadata":{"id":"E6WX9HnDAzrV"}},{"cell_type":"code","source":"subject_data = fetch_spm_auditory() \n\nprint(*subject_data.func[:5], sep=\"\\n\") ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:28.771245Z","iopub.execute_input":"2025-10-28T10:49:28.771487Z","iopub.status.idle":"2025-10-28T10:49:34.585519Z","shell.execute_reply.started":"2025-10-28T10:49:28.771467Z","shell.execute_reply":"2025-10-28T10:49:34.584195Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Check the dataset attributes\nprint(\"Dataset attributes:\")\nprint(subject_data.keys()) \n\n# Display the first 5 functional image files\nprint(\"\\nFirst 5 functional image files:\")\nprint(subject_data.func[:5])\n\n# Inspect the type and dimensions of the first file\nfirst_func_file = subject_data.func[0]\nprint(\"\\nType of the first functional file:\", type(first_func_file))\n\n# Load it with nibabel to inspect the image\nimg = nib.load(first_func_file)\nprint(\"Nibabel object type:\", type(img))\nprint(\"Image shape (x, y, z, t):\", img.shape)\nprint(\"Number of dimensions:\", img.ndim)\n\n# Display values\ndata = img.get_fdata()\nprint(\"Data type in the array:\", data.dtype)\nprint(\"Min/Max values in the image:\", data.min(), data.max())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:34.588227Z","iopub.execute_input":"2025-10-28T10:49:34.588503Z","iopub.status.idle":"2025-10-28T10:49:34.598381Z","shell.execute_reply.started":"2025-10-28T10:49:34.58848Z","shell.execute_reply":"2025-10-28T10:49:34.597431Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# How many 3D volumes has my 4D file?\nimg = nib.load(first_func_file)\nprint(\"Shape of the 4D file:\", img.shape)\nprint(\"Number of 3D volumes:\", img.shape[-1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:34.599424Z","iopub.execute_input":"2025-10-28T10:49:34.599738Z","iopub.status.idle":"2025-10-28T10:49:34.956358Z","shell.execute_reply.started":"2025-10-28T10:49:34.599711Z","shell.execute_reply":"2025-10-28T10:49:34.95541Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# List of all functional files for the subject\nfunc_files = subject_data.func  # ['fM00223_004.img', 'fM00223_005.img', ...]\n\n# Load each file as a Nifti1Image\nimgs_3d = [nib.load(f) for f in func_files]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:34.957287Z","iopub.execute_input":"2025-10-28T10:49:34.957538Z","iopub.status.idle":"2025-10-28T10:49:35.033641Z","shell.execute_reply.started":"2025-10-28T10:49:34.95752Z","shell.execute_reply":"2025-10-28T10:49:35.032611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img_4d = concat_imgs(imgs_3d)\nprint(\"Shape of the 4D merged file:\", img_4d.shape)  # (64,64,64,n_vols)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:35.034642Z","iopub.execute_input":"2025-10-28T10:49:35.034955Z","iopub.status.idle":"2025-10-28T10:49:35.256262Z","shell.execute_reply.started":"2025-10-28T10:49:35.034928Z","shell.execute_reply":"2025-10-28T10:49:35.255324Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Showing the first 3 volumes\nfor i in range(3):\n    vol_3d = index_img(img_4d, i)\n    plot_stat_map(\n        vol_3d,\n        title=f\"3D Volume {i+1}\",\n        cmap='viridis',\n        colorbar=True\n    )\n\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:35.25724Z","iopub.execute_input":"2025-10-28T10:49:35.257601Z","iopub.status.idle":"2025-10-28T10:49:40.039203Z","shell.execute_reply.started":"2025-10-28T10:49:35.257572Z","shell.execute_reply":"2025-10-28T10:49:40.038282Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"subject_data.anat","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:40.040191Z","iopub.execute_input":"2025-10-28T10:49:40.040428Z","iopub.status.idle":"2025-10-28T10:49:40.047416Z","shell.execute_reply.started":"2025-10-28T10:49:40.04041Z","shell.execute_reply":"2025-10-28T10:49:40.046406Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"anat_img = subject_data.anat\nplot_anat(anat_img, title=\"Anatomic Image\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:40.048711Z","iopub.execute_input":"2025-10-28T10:49:40.049033Z","iopub.status.idle":"2025-10-28T10:49:41.368116Z","shell.execute_reply.started":"2025-10-28T10:49:40.049005Z","shell.execute_reply":"2025-10-28T10:49:41.367265Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 4 : Visualize Data","metadata":{"id":"bYxVQD5mBKs2"}},{"cell_type":"markdown","source":"EDA (Exploratory Data Analysis) is about understanding the nature of your data.\nA quick way to do this is by using Pandas functions such as describe, info, shape, etc.\nHowever, this only applies when the data is structured â€” in other words, when it is organised in tables, like in Excel files.\n\nAnother way to understand your data is by visualising it.\nHere, we are going to use nilearn.plotting to visualise the subjectâ€™s data:","metadata":{"id":"jfd8GvEQBNHu"}},{"cell_type":"markdown","source":"#### Exercise 3. Visualise the brain using plot_anat and plot_img functions","metadata":{"id":"BsXuuA5_iJFT"}},{"cell_type":"markdown","source":"### Step 5. Concatenate images","metadata":{"id":"kfCFpM-YCG9Z"}},{"cell_type":"markdown","source":"We need to do this to concatenate all the 3D images into a single 4D image. After that, we calculate an average to create a background image, which we will later use to visualise the activations in different regions of the brain.","metadata":{"id":"4JKV0_9dIQNJ"}},{"cell_type":"markdown","source":"#### Exercise 4. Compute the mean of the fMRI image","metadata":{"id":"48Yez3yXlAjv"}},{"cell_type":"markdown","source":"For this, you should use the mean_img function from nilearn.image (already imported).","metadata":{"id":"-UYZiRbANqsQ"}},{"cell_type":"code","source":"# concat all the images 4D\nfmri_img = concat_imgs(subject_data.func)\nmean_img = mean_img(fmri_img)\n\nprint('Mean IMG:',mean_img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:41.369192Z","iopub.execute_input":"2025-10-28T10:49:41.369738Z","iopub.status.idle":"2025-10-28T10:49:41.844528Z","shell.execute_reply.started":"2025-10-28T10:49:41.36971Z","shell.execute_reply":"2025-10-28T10:49:41.843626Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_img(mean_img, title=\"Mean Brain (Mean fMRI)\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:41.845447Z","iopub.execute_input":"2025-10-28T10:49:41.845686Z","iopub.status.idle":"2025-10-28T10:49:42.44338Z","shell.execute_reply.started":"2025-10-28T10:49:41.845667Z","shell.execute_reply":"2025-10-28T10:49:42.442528Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 6: Create a table for the experiment events","metadata":{"id":"-1fziOmYCKtQ"}},{"cell_type":"markdown","source":"Now we need to provide a description of the experiment â€” that is, to define the timing of the auditory stimulation and the rest periods. This is usually provided in an events.tsv file. Fortunately, the path to this file is already included in the dataset we are working with.","metadata":{"id":"ebX2Td1QIKvJ"}},{"cell_type":"code","source":"events = pd.read_table(subject_data[\"events\"])\n# events.shape\nevents","metadata":{"id":"xxcHc7Xj72_U","executionInfo":{"status":"ok","timestamp":1729746731662,"user_tz":-120,"elapsed":330,"user":{"displayName":"Axel Casas","userId":"03871177800400842653"}},"outputId":"c36042bd-13f1-439d-89d0-096b546fc682","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:42.446795Z","iopub.execute_input":"2025-10-28T10:49:42.447049Z","iopub.status.idle":"2025-10-28T10:49:42.471677Z","shell.execute_reply.started":"2025-10-28T10:49:42.44703Z","shell.execute_reply":"2025-10-28T10:49:42.470954Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create the GLM model\nfmri_glm = FirstLevelModel(t_r=7, hrf_model='spm')\n\nfmri_glm = fmri_glm.fit(fmri_img, events=events)\n\n# Get the design matrix \ndesign_matrix = fmri_glm.design_matrices_[0]\n\nprint(\"Design matrix shape:\", design_matrix.shape)\nprint(design_matrix.head())\n\n# Plot the design matrix \nplot_design_matrix(design_matrix)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:42.472416Z","iopub.execute_input":"2025-10-28T10:49:42.47268Z","iopub.status.idle":"2025-10-28T10:49:45.269634Z","shell.execute_reply.started":"2025-10-28T10:49:42.472651Z","shell.execute_reply":"2025-10-28T10:49:45.268667Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 7. Detect voxels with significant effects","metadata":{"id":"eQhKblMsDx-m"}},{"cell_type":"markdown","source":"To access the estimated coefficients (the GLM model betas), we create a contrast with a single â€˜1â€™ in each of the relevant columns.\n\nThe purpose of the contrast is to select specific columns of the model in order to study the associated statistics.\n\nHere, we can define canonical contrasts that consider each effect in isolation â€” letâ€™s call them â€œconditionsâ€ â€” and then define a contrast that captures the difference between these conditions.","metadata":{"id":"9wAoC4TWFRCJ"}},{"cell_type":"code","source":"conditions = {\"active\": np.zeros(16), \"rest\": np.zeros(16)}\nconditions[\"active\"][0] = 1\nconditions[\"rest\"][1] = 1","metadata":{"id":"NCxqR_cm8wNL","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:45.270591Z","iopub.execute_input":"2025-10-28T10:49:45.270821Z","iopub.status.idle":"2025-10-28T10:49:45.275283Z","shell.execute_reply.started":"2025-10-28T10:49:45.270803Z","shell.execute_reply":"2025-10-28T10:49:45.274431Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we can compare the two conditions in the experiment (active vs. rest) by defining the corresponding contrast:","metadata":{"id":"Gcwb45J2FAN9"}},{"cell_type":"markdown","source":"#### Exercise 5: Create a variable called \"active_minus_rest\" that computes the difference between the \"active\" and \"rest\" conditions","metadata":{"id":"dvWqjT2qneXF"}},{"cell_type":"code","source":"active_minus_rest = conditions[\"active\"] - conditions[\"rest\"]","metadata":{"id":"GtQ88f8C8y4y","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:45.276323Z","iopub.execute_input":"2025-10-28T10:49:45.277221Z","iopub.status.idle":"2025-10-28T10:49:45.29199Z","shell.execute_reply.started":"2025-10-28T10:49:45.277198Z","shell.execute_reply":"2025-10-28T10:49:45.29114Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Exercise 6. Visualise the conditions in a Contrast Matrix","metadata":{"id":"HwFLGJY7n77H"}},{"cell_type":"code","source":"fmri_glm = FirstLevelModel(\n    t_r=7,\n    noise_model=\"ar1\",\n    standardize=False,\n    hrf_model=\"spm\",\n    drift_model=\"cosine\",\n    high_pass=0.01,\n)\n\nfmri_glm = fmri_glm.fit(fmri_img, events)\n\ndesign_matrix = fmri_glm.design_matrices_[0]","metadata":{"id":"Z4FykPwNCWpM","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:45.292864Z","iopub.execute_input":"2025-10-28T10:49:45.29319Z","iopub.status.idle":"2025-10-28T10:49:47.498917Z","shell.execute_reply.started":"2025-10-28T10:49:45.293169Z","shell.execute_reply":"2025-10-28T10:49:47.497919Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can use the function plot_contrast_matrix(data, design_matrix=design_matrix) to visualise:","metadata":{"id":"DhNMufSPoDNj"}},{"cell_type":"code","source":"print(design_matrix.columns)\nprint(design_matrix.shape)","metadata":{"id":"E2aSPHF781jM","executionInfo":{"status":"ok","timestamp":1729746928673,"user_tz":-120,"elapsed":634,"user":{"displayName":"Axel Casas","userId":"03871177800400842653"}},"outputId":"32ba3c36-1185-4955-9c30-e18732d2e43d","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:47.500056Z","iopub.execute_input":"2025-10-28T10:49:47.500643Z","iopub.status.idle":"2025-10-28T10:49:47.505796Z","shell.execute_reply.started":"2025-10-28T10:49:47.500614Z","shell.execute_reply":"2025-10-28T10:49:47.504976Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_contrast_matrix(active_minus_rest, design_matrix=design_matrix)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:47.506791Z","iopub.execute_input":"2025-10-28T10:49:47.507461Z","iopub.status.idle":"2025-10-28T10:49:47.781236Z","shell.execute_reply.started":"2025-10-28T10:49:47.50743Z","shell.execute_reply":"2025-10-28T10:49:47.780277Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next, we need to compute the â€œestimated effect.â€\n\nThis is measured in units of BOLD signal, but it doesnâ€™t provide strong statistical guarantees, since it doesnâ€™t account for the associated variance. In the following cells, you will see how this is done.","metadata":{"id":"h2ljQWL-FdQD"}},{"cell_type":"code","source":"eff_map = fmri_glm.compute_contrast(\n    active_minus_rest, output_type=\"effect_size\"\n)","metadata":{"id":"H_qloZrf81lv","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:47.78221Z","iopub.execute_input":"2025-10-28T10:49:47.782836Z","iopub.status.idle":"2025-10-28T10:49:47.965934Z","shell.execute_reply.started":"2025-10-28T10:49:47.782807Z","shell.execute_reply":"2025-10-28T10:49:47.96517Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To obtain statistical significance, we first need to create a t-statistic, which we will then convert to a z-score.\n\nThe z-score scale means that values are standardized to match a standard Gaussian distribution (mean = 0, variance = 1) across all voxels.","metadata":{"id":"b9uZwY2eFf13"}},{"cell_type":"code","source":"z_map = fmri_glm.compute_contrast(active_minus_rest, output_type=\"z_score\")","metadata":{"id":"dHwJ_vmJ81oT","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:47.966803Z","iopub.execute_input":"2025-10-28T10:49:47.967163Z","iopub.status.idle":"2025-10-28T10:49:48.195925Z","shell.execute_reply.started":"2025-10-28T10:49:47.967135Z","shell.execute_reply":"2025-10-28T10:49:48.195254Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Step 8. Visualise the Significant Results","metadata":{"id":"Ipq9L174FlCl"}},{"cell_type":"markdown","source":"âš ï¸ Important: remember that this dataset specifically focuses on the auditory cortex, not the DMN. Iâ€™ve created a plot below that you can use in your work, since the significant results will only indicate activation in the auditory cortex.","metadata":{"id":"j0lYeNs7Qr0y"}},{"cell_type":"markdown","source":"### ðŸ“š Some Notes\n#### Interpreting the Statistical Maps\n\nOnce we have generated the statistical maps, it is important to understand how to interpret the visualized results. In this case, we are working with Z-maps, where Z-values indicate how far the observed effect is from the mean, in terms of standard deviations.\n\n### What does a Z-value mean?\n\nPositive and significant Z-values: A Z-value greater than 3 (Z > 3) indicates significantly higher activation in a specific brain region compared to the reference condition (here, the rest state). For example, high Z-values in the auditory cortex suggest significant activation during the auditory task.\n\nNegative Z-values: Negative Z-values indicate deactivation in that region during the task compared to rest. Deactivated areas often include regions of the Default Mode Network (DMN), such as the Precuneus or Medial Prefrontal Cortex, which typically show higher activity during rest.\n\n### Statistical significance\n\nZ-values are scaled to a standard Gaussian distribution, allowing us to identify brain regions with significant activation. We typically apply a statistical threshold (e.g., Z > 3) to visualize only the regions showing significant activation. However, note that this threshold is arbitrary and does not account for the false positive rate (Type I errors).\n\nTo correct for this, stricter statistical corrections can be applied, such as False Discovery Rate (FDR) or Family-Wise Error Rate (FPR). For instance, an FDR correction with Î± = 0.05 reduces false positives and ensures the observed activations are more robust and reliable.\n\n### Activation Clusters\n\nFinally, small activation clusters that could be artifacts or noise are often removed. In this analysis, a 10-voxel threshold was applied, ensuring that only sufficiently large and significant clusters are considered. This helps identify the most relevant brain regions in terms of activation.","metadata":{"id":"W210GymqLxSt"}},{"cell_type":"markdown","source":"#### Exercise 7. Add Parameters to the plot_stat_map() Function","metadata":{"id":"I52PU5O4b73r"}},{"cell_type":"markdown","source":"For this example, we will arbitrarily use:\n\n1. a threshold of 3.0 in Z-score.\n\n2. `display_mode='z'`\n\n3. `cut_coords=3`","metadata":{"id":"iueB9j6YFvw2"}},{"cell_type":"code","source":"plot_stat_map(\n    z_map,\n    bg_img=mean_img,\n    threshold= 3.0,\n    display_mode= \"z\",\n    cut_coords= 3,\n    black_bg=True,\n    title= \"Active minus Rest (Z>3)\",\n)\nplt.show()","metadata":{"id":"d16uNn808-bG","executionInfo":{"status":"ok","timestamp":1729746964815,"user_tz":-120,"elapsed":3103,"user":{"displayName":"Axel Casas","userId":"03871177800400842653"}},"outputId":"8c3e4bef-9a55-4405-da37-6d6485404655","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:48.196897Z","iopub.execute_input":"2025-10-28T10:49:48.197174Z","iopub.status.idle":"2025-10-28T10:49:49.579822Z","shell.execute_reply.started":"2025-10-28T10:49:48.197154Z","shell.execute_reply":"2025-10-28T10:49:49.578906Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plot_stat_map(\n    z_map,\n    bg_img=mean_img,\n    threshold= 3.0,\n    display_mode= \"z\",\n    cut_coords= 3,\n    black_bg=True,\n    title= \"Active minus Rest (Z>3)\",\n)\nplt.show()","metadata":{"cellView":"form","id":"i2BgXbEiVIom","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:49.580787Z","iopub.execute_input":"2025-10-28T10:49:49.581492Z","iopub.status.idle":"2025-10-28T10:49:50.989753Z","shell.execute_reply.started":"2025-10-28T10:49:49.581462Z","shell.execute_reply":"2025-10-28T10:49:50.988895Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here we use an arbitrary threshold of 3.0, but the threshold should also take into account the risk of false detections (also known as Type I errors).\n\nOne suggestion is to control the false positive rate (FPR, denoted by alpha) at a certain level, for example 0.001. This means there is a 0.1% chance of declaring an inactive voxel as active.\n\nWe can do this as follows:","metadata":{"id":"16fnarXuGTgJ"}},{"cell_type":"code","source":"_, threshold = threshold_stats_img(z_map, alpha=0.001, height_control=\"fpr\")\nprint(f\"Uncorrected p<0.001 threshold: {threshold:.3f}\")\nplot_stat_map(\n    z_map,\n    bg_img=mean_img,\n    threshold=threshold,\n    display_mode=\"z\",\n    cut_coords=3,\n    black_bg=True,\n    title=\"Active minus Rest (p<0.001)\",\n)\nplt.show()","metadata":{"id":"acTPmmKV8-eG","executionInfo":{"status":"ok","timestamp":1729747080338,"user_tz":-120,"elapsed":2872,"user":{"displayName":"Axel Casas","userId":"03871177800400842653"}},"outputId":"32005ea3-7702-4837-f199-43235fdf9ddc","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:50.990781Z","iopub.execute_input":"2025-10-28T10:49:50.991234Z","iopub.status.idle":"2025-10-28T10:49:52.952615Z","shell.execute_reply.started":"2025-10-28T10:49:50.991212Z","shell.execute_reply":"2025-10-28T10:49:52.951555Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The problem is that with this approach, we expect 0.001 Ã— n_voxels false positives, even when they are inactive (which can be tens to hundreds of voxels).\n\nA popular alternative is to control the expected proportion of false discoveries among the detections, known as the false discovery rate (FDR).\n\nWe can do it as follows:","metadata":{"id":"7_aAySX9Gnq0"}},{"cell_type":"code","source":"_, threshold = threshold_stats_img(z_map, alpha=0.05, height_control=\"fdr\")\nprint(f\"False Discovery rate = 0.05 threshold: {threshold:.3f}\")\nplot_stat_map(\n    z_map,\n    bg_img=mean_img,\n    threshold=threshold,\n    display_mode=\"z\",\n    cut_coords=3,\n    black_bg=True,\n    title=\"Active minus Rest (fdr=0.05)\",\n)\nplt.show()","metadata":{"id":"xkduzK9m8-hJ","executionInfo":{"status":"ok","timestamp":1729747112600,"user_tz":-120,"elapsed":2863,"user":{"displayName":"Axel Casas","userId":"03871177800400842653"}},"outputId":"f90ed464-144b-4e76-ec3c-8aa34aa4977d","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:52.95365Z","iopub.execute_input":"2025-10-28T10:49:52.95422Z","iopub.status.idle":"2025-10-28T10:49:54.935462Z","shell.execute_reply.started":"2025-10-28T10:49:52.954197Z","shell.execute_reply":"2025-10-28T10:49:54.934526Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Finally, a good practice is to discard isolated voxels (also known as \"small clusters\") from these images.\n\nIt is possible to generate a thresholded map in which small clusters are removed by providing a cluster_threshold argument. In this case, clusters smaller than 10 voxels will be discarded.","metadata":{"id":"W0NddTseGlx4"}},{"cell_type":"code","source":"clean_map, threshold = threshold_stats_img(\n    z_map, alpha=0.05, height_control=\"fdr\", cluster_threshold=10\n)\nplot_stat_map(\n    clean_map,\n    bg_img=mean_img,\n    threshold=threshold,\n    display_mode=\"z\",\n    cut_coords=3,\n    black_bg=True,\n    title=\"Active minus Rest (fdr=0.05), clusters > 10 voxels\",\n)\nplt.show()","metadata":{"id":"FiQvdsrG9klw","executionInfo":{"status":"ok","timestamp":1729747132757,"user_tz":-120,"elapsed":4163,"user":{"displayName":"Axel Casas","userId":"03871177800400842653"}},"outputId":"8b8ec42d-6389-4b3b-c3e7-3f334d35a7d3","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:54.936536Z","iopub.execute_input":"2025-10-28T10:49:54.937188Z","iopub.status.idle":"2025-10-28T10:49:58.036569Z","shell.execute_reply.started":"2025-10-28T10:49:54.937158Z","shell.execute_reply":"2025-10-28T10:49:58.035598Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now we have a statistical analysis of the regions that are significantly more active compared to others.\n\nIn the image, we can see that there is very significant activity in the auditory cortex. This makes perfect sense, as the experimental task was to listen to a sound versus resting (doing nothing).","metadata":{"id":"PBuDCsiTWXLj"}},{"cell_type":"markdown","source":"#### Step 9: Save the results in a Table","metadata":{"id":"ZXSNWAWxG9VX"}},{"cell_type":"markdown","source":"To save the coordinates of significant results, itâ€™s good practice to have them organized and structured in a table.\n\nFortunately, we can do this very easily using the `get_clusters_table` function.","metadata":{"id":"k8C5N3lzHiwA"}},{"cell_type":"code","source":"table = get_clusters_table(\n    z_map, stat_threshold=threshold, cluster_threshold=20\n)\n\ntable","metadata":{"id":"SGmItSxdHPw0","executionInfo":{"status":"ok","timestamp":1729753307438,"user_tz":-120,"elapsed":2000,"user":{"displayName":"Axel Casas","userId":"03871177800400842653"}},"outputId":"27b641ba-35bb-4304-e619-1884f1cad5e8","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:58.037324Z","iopub.execute_input":"2025-10-28T10:49:58.037566Z","iopub.status.idle":"2025-10-28T10:49:59.295624Z","shell.execute_reply.started":"2025-10-28T10:49:58.037548Z","shell.execute_reply":"2025-10-28T10:49:59.29485Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 10: Write down the results","metadata":{"id":"cAm4WrP9HCvD"}},{"cell_type":"markdown","source":"#### Example 1","metadata":{"id":"amOxeTpbKq3O"}},{"cell_type":"markdown","source":"In our brain activation analysis for the listening versus rest task, we used a General Linear Model (GLM) to study differences in activation within the Default Mode Network (DMN). Through contrast analysis (listening vs. resting), a Z-map was generated showing significant brain activation for the auditory task.\n\nThe results indicated significantly higher activations in regions associated with the DMN when participants were at rest compared to the auditory task (Z > 3, p < 0.001 uncorrected). After applying a False Discovery Rate (FDR) correction with a threshold of 0.05, we observed that the most involved areas were concentrated in â€¦\n\nFinally, by removing small clusters of fewer than 10 voxels, we observed that the most significant activations remained in key areas of the DMN, suggesting a robust effect in these regions. The results are presented in Table 1, summarizing the coordinates of voxels with significant activation.\n\nFigure 1. Z-map of the differences between the auditory task and rest, showing significant activation in the DMN (Z > 3, p < 0.001).","metadata":{"id":"bqFnO9whJz5V"}},{"cell_type":"markdown","source":"#### Example 2","metadata":{"id":"Vgsy5i5WKsNF"}},{"cell_type":"markdown","source":"In this analysis, we implemented a General Linear Model (GLM) to compare brain activation during an auditory task versus a resting state, aiming to study the dynamics of the Default Mode Network (DMN). The data were analyzed using a block design with a TR of 7 seconds, and the canonical Hemodynamic Response Function (HRF) from SPM was applied to model the BOLD responses.\nThe results of the contrast between listening and resting conditions produced a Z-map reflecting significant activation across multiple brain regions. When defining the contrast â€œlisten minus rest,â€ robust activation was observed in bilateral auditory cortex areas during the auditory task. In contrast, relative deactivation was identified in regions belonging to the DMN, particularly the Precuneus, Medial Prefrontal Cortex (MPFC), and Inferior Parietal Cortex.\nAn estimated effects map was generated for the â€œlisten vs. restâ€ contrast, showing the magnitude of effects in BOLD signal units. However, these initial results did not provide full statistical validation, as the associated variance was not considered. To address this, a Z-map was calculated, allowing estimation of statistical significance based on variance. With a threshold of Z > 3, significant activations were detected in primary auditory cortex, while deactivations were observed in DMN areas.\nTo improve the reliability of the results and reduce the risk of false positives, two statistical corrections were applied. The control of the False Positive Rate (FPR) set a threshold of p < 0.001, ensuring that fewer than 0.1% of inactive voxels were incorrectly classified as active. Additionally, a False Discovery Rate (FDR) correction with Î± = 0.05 was implemented, focusing results on significant regions, particularly in the auditory cortex and DMN. Finally, a cluster threshold of 10 voxels removed small spurious clusters, consolidating the results in key brain regions.","metadata":{"id":"02bFKhjlKtTx"}},{"cell_type":"code","source":"# Create and fit the GLM\nfmri_glm = FirstLevelModel(t_r=7, hrf_model='spm')\nfmri_glm = fmri_glm.fit(fmri_img, events=events)\n\n# Check the design matrix column names\ndesign_matrix = fmri_glm.design_matrices_[0]\nprint(\"Design matrix columns:\", design_matrix.columns)\n\n# Create the contrast 'active minus rest'\ncontrast_vector = np.array([1, -1, 0])\n\n# Compute the contrast as a Z-score map\nz_map = fmri_glm.compute_contrast(contrast_vector, output_type='z_score')\n\nplot_stat_map(z_map, threshold=3.0, display_mode='z', cut_coords=3, title='Active minus Rest')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:49:59.296592Z","iopub.execute_input":"2025-10-28T10:49:59.297252Z","iopub.status.idle":"2025-10-28T10:50:03.39627Z","shell.execute_reply.started":"2025-10-28T10:49:59.297224Z","shell.execute_reply":"2025-10-28T10:50:03.395343Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Step 11: Work figures","metadata":{"id":"AnpDZ1R5WlNf"}},{"cell_type":"code","source":"import numpy as np\nimport nibabel as nib\nfrom nilearn import plotting, datasets\nfrom nilearn.image import threshold_img\n\n# Step 1: Define prefrontal cortex coordinates and peak Z-statistics\ncoordinates_and_peaks = [\n    (-60, -6, 42, 9.81),  # MNI coordinates of prefrontal cortex with peak Z-stat values\n    (-63, 6, 36, 8.60),\n    (-63, 0, 42, 8.43),\n    (-48, -15, 39, 8.36),\n    (45, -12, 42, 7.59)\n]\n\n# Step 2: Create an empty brain image (MNI space: 91x109x91) with a basic affine\nbrain_shape = (91, 109, 91)\naffine = np.array([[2., 0., 0., -90.],\n                   [0., 2., 0., -126.],\n                   [0., 0., 2., -72.],\n                   [0., 0., 0., 1.]])\nbrain_data = np.zeros(brain_shape)\n\n# Step 3: Place Z-statistic values into the brain image at the MNI coordinates\nfor x, y, z, z_value in coordinates_and_peaks:\n    voxel_coord = np.round(np.linalg.inv(affine).dot([x, y, z, 1]))[:3].astype(int)\n    # Check bounds to avoid indexing errors\n    if all(0 <= voxel_coord[i] < brain_shape[i] for i in range(3)):\n        brain_data[tuple(voxel_coord)] = z_value\n\n# Step 4: Create a Nifti image for the simulated activations\nsimulated_img = nib.Nifti1Image(brain_data, affine)\n\n# Step 5: Apply a simple statistical threshold (uncorrected p<0.001 ~ Z>3.09)\nthreshold = 3.09\nthresholded_img = threshold_img(simulated_img, threshold=threshold)\n\n# Step 6: Load the MNI template as the background image\nmean_img = datasets.load_mni152_template()\n\n# Step 7: Visualize the statistical map with thresholding\nplotting.plot_stat_map(\n    thresholded_img,\n    bg_img=mean_img,      # Use MNI template as background\n    threshold=threshold,  # Apply the threshold\n    display_mode=\"z\",\n    cut_coords=(0, 50, 10),  # Focus on prefrontal cortex\n    black_bg=True,        # Black background for clarity\n    title=\"Simulated Prefrontal Cortex Activation\"\n)\n\nplotting.show()\n","metadata":{"cellView":"form","id":"EcjfGoAuXiBt","executionInfo":{"status":"error","timestamp":1729770523982,"user_tz":-120,"elapsed":1468,"user":{"displayName":"Axel Casas","userId":"03871177800400842653"}},"outputId":"84c2a06e-4785-4a58-d15d-f59d04901fd7","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:50:03.397036Z","iopub.execute_input":"2025-10-28T10:50:03.39729Z","iopub.status.idle":"2025-10-28T10:50:05.781271Z","shell.execute_reply.started":"2025-10-28T10:50:03.397271Z","shell.execute_reply":"2025-10-28T10:50:05.780255Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Step 1: Define the prefrontal cortex coordinates and peak activations\ncoordinates_and_peaks = [\n    (-60, -6, 42, 9.81),  # Example prefrontal cortex MNI coordinates\n    (-63, 6, 36, 8.60),\n    (-63, 0, 42, 8.43),\n    (-48, -15, 39, 8.36),\n    (45, -12, 42, 7.59)  # You can replace these with the prefrontal activations from your table\n]\n\n# Step 2: Create an empty brain image (MNI space: 91x109x91) with a basic affine\nbrain_shape = (91, 109, 91)\naffine = np.array([[2., 0., 0., -90.],\n                   [0., 2., 0., -126.],\n                   [0., 0., 2., -72.],\n                   [0., 0., 0., 1.]])\nbrain_data = np.zeros(brain_shape)\n\n# Step 3: Place peak activations into the brain image at the MNI coordinates\nfor x, y, z, peak in coordinates_and_peaks:\n    voxel_coord = np.round(np.linalg.inv(affine).dot([x, y, z, 1]))[:3].astype(int)\n    brain_data[tuple(voxel_coord)] = peak\n\n# Step 4: Create a Nifti image for the simulated activations\nsimulated_img = nib.Nifti1Image(brain_data, affine)\n\nplotting.plot_stat_map(simulated_img, display_mode='ortho', threshold=3.0,\n                       title=\"\", cut_coords=(0, 50, 10))\n\nplotting.plot_glass_brain(simulated_img, threshold=3.0, display_mode='lzry', colorbar=True,\n                          title=\"\")\n\nplotting.show()","metadata":{"cellView":"form","id":"dYg3q9l1Vf56","executionInfo":{"status":"ok","timestamp":1729753734090,"user_tz":-120,"elapsed":9830,"user":{"displayName":"Axel Casas","userId":"03871177800400842653"}},"outputId":"c7c16499-9d77-424e-d93e-8683c9430538","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:50:05.782232Z","iopub.execute_input":"2025-10-28T10:50:05.782531Z","iopub.status.idle":"2025-10-28T10:50:09.101811Z","shell.execute_reply.started":"2025-10-28T10:50:05.782509Z","shell.execute_reply":"2025-10-28T10:50:09.100875Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### DMN Graphs","metadata":{"id":"ScAXb9S4WQGR"}},{"cell_type":"markdown","source":"In this project, it was decided to use the DiFuMo atlas (or Harvardâ€“Oxford, depending on the case) instead of the original MSDL atlas. The main reason is that the MSDL atlas, although widely used for identifying networks such as the Default Mode Network (DMN), is no longer available on its official servers, and there are no reliable public mirrors for downloading it.\nAttempts to use fetch_atlas_msdl() resulted in connection errors due to network blocks and the fact that the original files have been removed.\nFor this reason, an alternative functional atlas (DiFuMo) was chosen because it:\n\n\nIs available for download from accessible and reliable servers (OSF),\n\n\nIs also functional and probabilistic, allowing identification of brain networks similarly to MSDL,\n\n\nEnsures reproducibility and compatibility with environments such as Kaggle.\n\n\nThis choice guarantees that analyses and visualizations can be performed without download errors or incompatibilities, while maintaining the functional focus originally intended.","metadata":{}},{"cell_type":"code","source":"# # Downloas the MSDL atlas\n# msdl_atlas = datasets.fetch_atlas_msdl()\n\n# # This is the link\n# https://team.inria.fr/parietal/files/2015/01/MSDL_rois.zip","metadata":{"id":"EX3kvMncRDy5","executionInfo":{"status":"ok","timestamp":1729752227874,"user_tz":-120,"elapsed":5314,"user":{"displayName":"Axel Casas","userId":"03871177800400842653"}},"outputId":"69cd0e6e-3044-4790-bf6d-c2764bcd1f89","trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:52:58.719779Z","iopub.execute_input":"2025-10-28T10:52:58.720535Z","iopub.status.idle":"2025-10-28T10:52:58.724509Z","shell.execute_reply.started":"2025-10-28T10:52:58.720505Z","shell.execute_reply":"2025-10-28T10:52:58.723414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# # Extract the corresponding nodes to the Default Mode Network (DMN)\n# dmn_nodes = image.index_img(msdl_atlas.maps, [3, 4, 5, 6])\n\n# # Visualize the nodes from DMN\n# plotting.plot_prob_atlas(dmn_nodes, cut_coords=(0, -60, 29), draw_cross=False,\n#                          annotate=False, title=\"DMN nodes in MSDL atlas\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:52:58.967839Z","iopub.execute_input":"2025-10-28T10:52:58.968213Z","iopub.status.idle":"2025-10-28T10:52:58.972761Z","shell.execute_reply.started":"2025-10-28T10:52:58.968189Z","shell.execute_reply":"2025-10-28T10:52:58.971889Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"atlas = datasets.fetch_atlas_harvard_oxford('cort-maxprob-thr25-2mm')\nplotting.plot_prob_atlas(atlas.maps, title=\"Harvard-Oxford Atlas\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:52:59.879742Z","iopub.execute_input":"2025-10-28T10:52:59.88032Z","iopub.status.idle":"2025-10-28T10:53:01.078245Z","shell.execute_reply.started":"2025-10-28T10:52:59.880293Z","shell.execute_reply":"2025-10-28T10:53:01.076865Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from nilearn import image, datasets, plotting\n\n# Get the DiFuMo dataset\ndifumo = datasets.fetch_atlas_difumo()\n\n# difumo.maps is a Nifti image with all the regions\n# We choose some indexes as example\ndmn_nodes = image.index_img(difumo.maps, [0, 1, 2, 3])\n\nplotting.plot_prob_atlas(\n    dmn_nodes, cut_coords=(0, -60, 29),\n    draw_cross=False, annotate=False,\n    title=\"DMN nodes (DiFuMo atlas)\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:53:43.645633Z","iopub.execute_input":"2025-10-28T10:53:43.645948Z","iopub.status.idle":"2025-10-28T10:53:47.958545Z","shell.execute_reply.started":"2025-10-28T10:53:43.645924Z","shell.execute_reply":"2025-10-28T10:53:47.957596Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract the nodes corresponding to the Default Mode Network (DMN)\ndmn_nodes = image.index_img(difumo.maps, [3, 4, 5, 6])\n\n# Visualize the DMN nodes\nplotting.plot_prob_atlas(\n    dmn_nodes, \n    cut_coords=(0, -60, 29), \n    draw_cross=False,\n    annotate=False, \n    title=\"DMN nodes in MSDL atlas\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-28T10:53:52.016471Z","iopub.execute_input":"2025-10-28T10:53:52.016806Z","iopub.status.idle":"2025-10-28T10:53:56.225656Z","shell.execute_reply.started":"2025-10-28T10:53:52.016763Z","shell.execute_reply":"2025-10-28T10:53:56.224755Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"| Atlas          | Type       | Functionality                             | Safe to download on Kaggle? |\n| -------------- | ---------- | ----------------------------------------- | --------------------------- |\n| **DiFuMo**     | Functional | Functional networks, probabilistic        | Yes                         |\n| **Schaefer**   | Functional | Functional networks, multiple resolutions | Yes                         |\n| Harvardâ€“Oxford | Anatomical | Structural regions                        | Yes                         |\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}